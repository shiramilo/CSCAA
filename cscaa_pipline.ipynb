{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0b40e7e",
   "metadata": {},
   "source": [
    "# Complete Sattistical Correlation and Association Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635b8228",
   "metadata": {},
   "source": [
    "## Stat test selection based on data distribution and data type:\n",
    "\n",
    "### Numerical data\n",
    "##### Pearson correlation if both variables are normally distributed\n",
    "##### Spearman correlation if at least one variable is not normal.\n",
    "\n",
    "### Categorical data vs. numerical data\n",
    "##### Mann-Whitney U test for binary categorical data\n",
    "##### Kruskal-Wallis test for multi-category comparisons\n",
    "\n",
    "### Categorical data vs. categorical data\n",
    "##### Chi-square test for general associations\n",
    "##### Fisherâ€™s Exact Test if expected frequencies are low\n",
    "\n",
    "### False Discovery Rate (FDR) adjustment to reduce false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02f0323c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from statsmodels.stats.diagnostic import lilliefors\n",
    "import os\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(\"data_dist\", exist_ok=True)\n",
    "os.makedirs(\"norm_res\", exist_ok=True)\n",
    "os.makedirs(\"corr_res\", exist_ok=True)\n",
    "os.makedirs(\"violin_plots\", exist_ok=True)\n",
    "os.makedirs(\"corr_plot\", exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "data_file_path = \"input/all_cat_for_corr.csv\"\n",
    "data = pd.read_csv(data_file_path, encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = [col for col in data.columns if col.endswith(\"CAT\")]\n",
    "boolean_cols = data.select_dtypes(include=['bool']).columns.tolist()\n",
    "categorical_cols += [col for col in boolean_cols if col not in categorical_cols]\n",
    "\n",
    "numerical_cols = [col for col in data.columns if col not in categorical_cols and col != \"strain_id\"]\n",
    "\n",
    "# Convert categorical data properly (including boolean columns)\n",
    "data[categorical_cols] = data[categorical_cols].astype(\"category\")\n",
    "\n",
    "# Convert numerical data properly\n",
    "data[numerical_cols] = data[numerical_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Remove infinite values from numerical data\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# **Step 1: Normality Testing & Distribution Plots**\n",
    "normality_results = {}\n",
    "for column in numerical_cols:\n",
    "    if data[column].nunique() > 1:\n",
    "        plt.figure()\n",
    "        sns.histplot(data[column].dropna(), kde=True)\n",
    "        plt.title(f'Distribution of {column}')\n",
    "        plt.savefig(f'data_dist/distribution_{column}.png')\n",
    "        plt.close()\n",
    "\n",
    "        try:\n",
    "            sample_size = len(data[column].dropna())\n",
    "            if sample_size < 500:\n",
    "                stat, p = stats.shapiro(data[column].dropna())\n",
    "                test_type = \"Shapiro-Wilk\"\n",
    "            else:\n",
    "                stat, p = lilliefors(data[column].dropna())\n",
    "                test_type = \"Lilliefors\"\n",
    "\n",
    "            normality_results[column] = {'Test': test_type, 'Statistic': stat, 'p-value': p}\n",
    "\n",
    "        except Exception:\n",
    "            normality_results[column] = {'Test': 'Error', 'Statistic': 'Error', 'p-value': 'Error'}\n",
    "\n",
    "pd.DataFrame(normality_results).T.to_csv('norm_res/normality_results.csv')\n",
    "\n",
    "# **Step 2: Correlations (Num-Num and Cat-Num)**\n",
    "results = []\n",
    "p_values = []  \n",
    "processed_pairs = set()\n",
    "\n",
    "for phenotype in numerical_cols:\n",
    "    for metadata in categorical_cols + numerical_cols:\n",
    "        if phenotype == metadata or (metadata, phenotype) in processed_pairs:\n",
    "            continue\n",
    "\n",
    "        processed_pairs.add((phenotype, metadata))\n",
    "\n",
    "        valid_data = data[[phenotype, metadata]].dropna()\n",
    "        if valid_data.shape[0] <= 5:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if metadata in numerical_cols:\n",
    "                normal_p1 = normality_results.get(phenotype, {}).get('p-value', 1)\n",
    "                normal_p2 = normality_results.get(metadata, {}).get('p-value', 1)\n",
    "\n",
    "                if normal_p1 > 0.05 and normal_p2 > 0.05:\n",
    "                    stat, p = stats.pearsonr(valid_data[metadata], valid_data[phenotype])\n",
    "                    test_type = \"Pearson\"\n",
    "                else:\n",
    "                    stat, p = stats.spearmanr(valid_data[metadata], valid_data[phenotype])\n",
    "                    test_type = \"Spearman\"\n",
    "\n",
    "            else:\n",
    "                filtered_groups = [valid_data[valid_data[metadata] == cat][phenotype].dropna() \n",
    "                                   for cat in valid_data[metadata].unique() \n",
    "                                   if len(valid_data[valid_data[metadata] == cat]) > 5]\n",
    "\n",
    "                if len(filtered_groups) < 2:\n",
    "                    continue\n",
    "\n",
    "                if len(filtered_groups) == 2:\n",
    "                    stat, p = stats.mannwhitneyu(*filtered_groups, alternative=\"two-sided\", method=\"asymptotic\")\n",
    "                    test_type = \"Mann-Whitney U\"\n",
    "                else:\n",
    "                    stat, p = stats.kruskal(*filtered_groups)\n",
    "                    test_type = \"Kruskal-Wallis\"\n",
    "\n",
    "            if np.isnan(stat) or np.isnan(p):\n",
    "                continue\n",
    "\n",
    "            results.append({'Metadata': metadata, 'Phenotype': phenotype, 'Test': test_type, 'Statistic': stat, 'p-value': p})\n",
    "            p_values.append(p)\n",
    "\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "# **Categorical vs. Categorical (Chi-square & Fisher's Exact)**\n",
    "for i, cat1 in enumerate(categorical_cols):\n",
    "    for cat2 in categorical_cols[i+1:]:\n",
    "        valid_data = data[[cat1, cat2]].dropna()\n",
    "        if valid_data.shape[0] <= 5:\n",
    "            continue\n",
    "\n",
    "        contingency_table = pd.crosstab(valid_data[cat1], valid_data[cat2])\n",
    "        if contingency_table.shape[0] < 2 or contingency_table.shape[1] < 2:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if (contingency_table.values < 5).any() and contingency_table.size == 4:\n",
    "                stat, p = stats.fisher_exact(contingency_table)\n",
    "                test_type = \"Fisher's Exact Test\"\n",
    "            else:\n",
    "                stat, p, _, _ = stats.chi2_contingency(contingency_table)\n",
    "                test_type = \"Chi-square Test\"\n",
    "\n",
    "            results.append({'Metadata': cat1, 'Phenotype': cat2, 'Test': test_type, 'Statistic': stat, 'p-value': p})\n",
    "            p_values.append(p)\n",
    "\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "# **Apply FDR Correction**\n",
    "if p_values:\n",
    "    _, corrected_p_values, _, _ = multipletests(p_values, method='fdr_bh')\n",
    "    for i, result in enumerate(results):\n",
    "        result['corrected p-value'] = corrected_p_values[i]\n",
    "else:\n",
    "    for result in results:\n",
    "        result['corrected p-value'] = 'NA'\n",
    "\n",
    "pd.DataFrame(results).to_csv('corr_res/correlation_results_with_fdr.csv', index=False)\n",
    "\n",
    "# **Step 3: Clustered Heatmap**\n",
    "if len(numerical_cols) > 1:\n",
    "    correlation_matrix = data[numerical_cols].corr(method='spearman')\n",
    "    clustergrid = sns.clustermap(correlation_matrix, cmap=\"BrBG\", linewidths=0.1, figsize=(60, 50), annot=False, method='average')\n",
    "    clustergrid.fig.suptitle(\"Clustered Numerical Correlation Heatmap\", fontsize=24)\n",
    "    plt.savefig(\"corr_plot/numerical_correlation_clustered.png\")\n",
    "    plt.close()\n",
    "\n",
    "# **Step 4: Violin plots with significance and log-transform for MIC variables**\n",
    "for metadata in categorical_cols:\n",
    "    for phenotype in numerical_cols:\n",
    "        valid_data = data[[metadata, phenotype]].dropna().copy()\n",
    "        \n",
    "        # Log-transform values if phenotype contains 'MIC'\n",
    "        if \"MIC\" in phenotype.upper():\n",
    "            valid_data[phenotype] = np.log10(valid_data[phenotype].replace(0, np.nan).dropna())\n",
    "\n",
    "        if valid_data.shape[0] > 5 and valid_data[metadata].nunique() in [2, 3, 4, 5]:\n",
    "            plt.figure(figsize=(12, 9))\n",
    "            ax = sns.violinplot(\n",
    "                x=valid_data[metadata].astype(str), \n",
    "                y=valid_data[phenotype], \n",
    "                hue=valid_data[metadata].astype(str),\n",
    "                palette=\"Accent\", \n",
    "                inner=\"box\"\n",
    "            )\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.title(f'{metadata} vs. {phenotype}' + (' (log10-transformed)' if 'MIC' in phenotype.upper() else ''))\n",
    "\n",
    "            groups = [valid_data[valid_data[metadata] == cat][phenotype].dropna() \n",
    "                      for cat in valid_data[metadata].unique()]\n",
    "\n",
    "            if len(groups) == 2:\n",
    "                _, p_value = stats.mannwhitneyu(*groups, method=\"asymptotic\")\n",
    "            else:\n",
    "                _, p_value = stats.kruskal(*groups)\n",
    "\n",
    "            ax.text(0.5, ax.get_ylim()[1] * 0.95, f\"p = {p_value:.3g}\",\n",
    "                    ha='center', fontsize=12, color='black')\n",
    "\n",
    "            plt.savefig(f'violin_plots/{metadata}_vs_{phenotype}.png')\n",
    "            plt.close()\n",
    "\n",
    "print(\"Analysis completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfc8f69",
   "metadata": {},
   "source": [
    "## Post-Hoc tests for significant correlation/associations:\n",
    "\n",
    "### Cutoff: FDR < 0.05\n",
    "\n",
    "### Pairwise Chi-square for significant Chi-Square Test results\n",
    "### Pairwise Mann-Whitney U for significant Mann-Whitney U results\n",
    "### Dunn's Test for significant Kruskal-Wallis results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe4558c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-hoc analysis completed successfully! 12770 comparisons saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import scikit_posthocs as sp\n",
    "import os\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(\"posthoc_res\", exist_ok=True)\n",
    "\n",
    "# Load correlation results\n",
    "data = pd.read_csv('corr_res/correlation_results_with_fdr.csv')\n",
    "\n",
    "# Load original dataset (single file input)\n",
    "original_data = pd.read_csv('input/all_cat_for_corr.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Initialize list for storing post-hoc results\n",
    "posthoc_results = []\n",
    "\n",
    "# Iterate over significant results\n",
    "for _, row in data.iterrows():\n",
    "    metadata, phenotype, test, p, fdr_p = row['Metadata'], row['Phenotype'], row['Test'], row['p-value'], row['corrected p-value']\n",
    "    \n",
    "    # Apply post-hoc only if the FDR-adjusted p-value is significant\n",
    "    if fdr_p < 0.05:\n",
    "        valid_data = original_data[[phenotype, metadata]].dropna()\n",
    "        valid_data[metadata] = valid_data[metadata].astype(str)  # <-- FIX HERE\n",
    "\n",
    "        try:\n",
    "            if test == 'Chi-square Test':\n",
    "                contingency_table = pd.crosstab(valid_data[metadata], valid_data[phenotype])\n",
    "                comparisons_done = set()\n",
    "                for row_label in contingency_table.index:\n",
    "                    for col_label in contingency_table.columns:\n",
    "                        if row_label != col_label:\n",
    "                            comparison_key = tuple(sorted([str(row_label), str(col_label)]))\n",
    "                            if comparison_key not in comparisons_done:\n",
    "                                comparisons_done.add(comparison_key)\n",
    "                                observed = np.array([\n",
    "                                    [contingency_table.loc[row_label, col_label], sum(contingency_table.loc[row_label, :]) - contingency_table.loc[row_label, col_label]],\n",
    "                                    [sum(contingency_table[col_label]) - contingency_table.loc[row_label, col_label],\n",
    "                                     contingency_table.values.sum() - sum(contingency_table.loc[row_label, :]) - sum(contingency_table[col_label]) + contingency_table.loc[row_label, col_label]]\n",
    "                                ])\n",
    "                                chi2, post_p, _, _ = stats.chi2_contingency(observed)\n",
    "                                group1_count = observed[0].sum()\n",
    "                                group2_count = observed[1].sum()\n",
    "                                posthoc_results.append({\n",
    "                                    'Metadata': metadata,\n",
    "                                    'Phenotype': phenotype,\n",
    "                                    'Comparison': f'{row_label} vs {col_label}',\n",
    "                                    'Post Hoc Test': 'Pairwise Chi-square',\n",
    "                                    'p-value': post_p,\n",
    "                                    'Group1_Count': group1_count,\n",
    "                                    'Group2_Count': group2_count\n",
    "                                })\n",
    "\n",
    "            elif test == 'Mann-Whitney U':\n",
    "                categories = list(valid_data[metadata].unique())\n",
    "                comparisons_done = set()\n",
    "                for i in range(len(categories)):\n",
    "                    for j in range(i + 1, len(categories)):\n",
    "                        comparison_key = tuple(sorted([categories[i], categories[j]]))\n",
    "                        if comparison_key not in comparisons_done:\n",
    "                            comparisons_done.add(comparison_key)\n",
    "                            group1 = valid_data[valid_data[metadata] == categories[i]][phenotype]\n",
    "                            group2 = valid_data[valid_data[metadata] == categories[j]][phenotype]\n",
    "                            stat, post_p = stats.mannwhitneyu(group1, group2, alternative='two-sided')\n",
    "                            group1_median = np.median(group1)\n",
    "                            group2_median = np.median(group2)\n",
    "                            posthoc_results.append({\n",
    "                                'Metadata': metadata,\n",
    "                                'Phenotype': phenotype,\n",
    "                                'Comparison': f'{categories[i]} vs {categories[j]}',\n",
    "                                'Post Hoc Test': 'Pairwise Mann-Whitney U',\n",
    "                                'p-value': post_p,\n",
    "                                'Group1_Median': group1_median,\n",
    "                                'Group2_Median': group2_median\n",
    "                            })\n",
    "\n",
    "            elif test == 'Kruskal-Wallis':\n",
    "                posthoc = sp.posthoc_dunn(valid_data, val_col=phenotype, group_col=metadata, p_adjust='bonferroni')\n",
    "                comparisons_done = set()\n",
    "                for group1 in posthoc.index:\n",
    "                    for group2 in posthoc.columns:\n",
    "                        if group1 != group2:\n",
    "                            comparison_key = tuple(sorted([group1, group2]))\n",
    "                            if comparison_key not in comparisons_done:\n",
    "                                comparisons_done.add(comparison_key)\n",
    "                                group1_vals = valid_data[valid_data[metadata] == group1][phenotype]\n",
    "                                group2_vals = valid_data[valid_data[metadata] == group2][phenotype]\n",
    "                                group1_median = np.median(group1_vals)\n",
    "                                group2_median = np.median(group2_vals)\n",
    "                                posthoc_results.append({\n",
    "                                    'Metadata': metadata,\n",
    "                                    'Phenotype': phenotype,\n",
    "                                    'Comparison': f'{group1} vs {group2}',\n",
    "                                    'Post Hoc Test': \"Dunn's Test\",\n",
    "                                    'p-value': posthoc.loc[group1, group2],\n",
    "                                    'Group1_Median': group1_median,\n",
    "                                    'Group2_Median': group2_median\n",
    "                                })\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping post-hoc for {metadata} vs {phenotype} due to error: {e}\")\n",
    "\n",
    "# Save posthoc results\n",
    "posthoc_df = pd.DataFrame(posthoc_results)\n",
    "posthoc_df.to_csv('posthoc_res/posthoc_results.csv', index=False)\n",
    "\n",
    "print(f'Post-hoc analysis completed successfully! {len(posthoc_results)} comparisons saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7463a5ed",
   "metadata": {},
   "source": [
    "## Post-Hoc results interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c21baa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-hoc interpretation file with fold change created successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load significant post-hoc results\n",
    "posthoc_df = pd.read_csv('posthoc_res/posthoc_results.csv')\n",
    "\n",
    "# Filter for significant results only\n",
    "significant_results = posthoc_df[posthoc_df[\"p-value\"] < 0.05].copy()\n",
    "\n",
    "# Define interpretation function\n",
    "def interpret_row(row):\n",
    "    metadata = row['Metadata'].replace(\"_CAT\", \"\")\n",
    "    phenotype = row['Phenotype']\n",
    "    comparison = row['Comparison']\n",
    "    p_val = row['p-value']\n",
    "    group1, group2 = comparison.split(' vs ')\n",
    "    test_type = row['Post Hoc Test']\n",
    "\n",
    "    # Decide if we deal with medians or counts\n",
    "    if 'Group1_Median' in row and not pd.isna(row['Group1_Median']):\n",
    "        median1 = row['Group1_Median']\n",
    "        median2 = row['Group2_Median']\n",
    "\n",
    "        if median1 > median2:\n",
    "            interpretation = (f\"'{phenotype}': '{group1}' shows significantly higher values \"\n",
    "                              f\"than '{group2}' in '{metadata}' (median: {median1:.3f} vs {median2:.3f}; p={p_val:.2e}).\")\n",
    "            comparison_direction = f\"{group1} > {group2}\"\n",
    "            fold_change = abs(median1) / abs(median2) if median2 != 0 else np.inf\n",
    "\n",
    "        elif median2 > median1:\n",
    "            interpretation = (f\"'{phenotype}': '{group2}' shows significantly higher values \"\n",
    "                              f\"than '{group1}' in '{metadata}' (median: {median2:.3f} vs {median1:.3f}; p={p_val:.2e}).\")\n",
    "            comparison_direction = f\"{group2} > {group1}\"\n",
    "            fold_change = abs(median2) / abs(median1) if median1 != 0 else np.inf\n",
    "\n",
    "        else:\n",
    "            interpretation = (f\"'{phenotype}': '{group1}' and '{group2}' in '{metadata}' have identical medians ({median1:.3f}), \"\n",
    "                              f\"but differ significantly (p={p_val:.2e}).\")\n",
    "            comparison_direction = f\"{group1} = {group2}\"\n",
    "            fold_change = 1.0\n",
    "\n",
    "    elif 'Group1_Count' in row and not pd.isna(row['Group1_Count']):\n",
    "        count1 = row['Group1_Count']\n",
    "        count2 = row['Group2_Count']\n",
    "\n",
    "        # Adjusted interpretation for categorical vs categorical tests\n",
    "        interpretation = (f\"There is a significant association between '{metadata}' ('{group1}') \"\n",
    "                          f\"and '{phenotype}' ('{group2}') (counts: {count1} vs {count2}; p={p_val:.2e}).\")\n",
    "        comparison_direction = f\"{metadata}:{group1} â†” {phenotype}:{group2}\"\n",
    "        fold_change = count1 / count2 if count2 != 0 else np.inf\n",
    "\n",
    "    else:\n",
    "        interpretation = \"Insufficient data\"\n",
    "        comparison_direction = \"N/A\"\n",
    "        fold_change = np.nan\n",
    "\n",
    "    return pd.Series({\n",
    "        \"Comparison_Direction\": comparison_direction,\n",
    "        \"Fold_Change\": fold_change,\n",
    "        \"Interpretation\": interpretation\n",
    "    })\n",
    "\n",
    "# Apply interpretation\n",
    "interpretation_results = significant_results.apply(interpret_row, axis=1)\n",
    "\n",
    "# Insert 'Comparison_Direction' and 'Fold_Change' columns after 'Group2_Count'\n",
    "insert_pos = significant_results.columns.get_loc(\"Group2_Count\") + 1\n",
    "significant_results.insert(loc=insert_pos, column=\"Comparison_Direction\",\n",
    "                           value=interpretation_results[\"Comparison_Direction\"])\n",
    "significant_results.insert(loc=insert_pos + 1, column=\"Fold_Change\",\n",
    "                           value=interpretation_results[\"Fold_Change\"])\n",
    "\n",
    "# Add Interpretation as last column\n",
    "significant_results[\"Interpretation\"] = interpretation_results[\"Interpretation\"]\n",
    "\n",
    "# Save results\n",
    "significant_results.to_csv('posthoc_res/posthoc_interpretation.csv', index=False)\n",
    "\n",
    "print(\"Post-hoc interpretation file with fold change created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed13192-7464-4c35-bb4f-40592bb1116b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
